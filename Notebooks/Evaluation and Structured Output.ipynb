{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84d0e5d",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f37ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05484a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['google_api_key']  = os.getenv('google_api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dc7e5",
   "metadata": {},
   "source": [
    "Automated retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6593c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd18a4a",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72648a9",
   "metadata": {},
   "source": [
    "When using LLMs in real-world cases, it's important to understand how well they are performing. The open-ended generation capabilities of LLMs can make many cases difficult to measure. In this notebook you will walk through some simple techniques for evaluating LLM outputs and understanding their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ac60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  1 7059k    1 83606    0     0   297k      0  0:00:23 --:--:--  0:00:23  299k\n",
      " 53 7059k   53 3802k    0     0  2985k      0  0:00:02  0:00:01  0:00:01 2987k\n",
      "100 7059k  100 7059k    0     0  3309k      0  0:00:02  0:00:02 --:--:-- 3312k\n"
     ]
    }
   ],
   "source": [
    "!curl -o gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b37289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PDF has 77 pages\n",
      "Extracted 10003 characters of text\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "pdf_path = 'gemini.pdf'\n",
    "# Open the PDF file\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "# Get the number of pages\n",
    "num_pages = len(pdf_reader.pages)\n",
    "print(f\"The PDF has {num_pages} pages\")\n",
    "\n",
    "# Extract text from first page as an example\n",
    "first_page = pdf_reader.pages[0]\n",
    "text = first_page.extract_text()\n",
    "\n",
    "# Extract text from multiple pages (adjust range as needed)\n",
    "full_text = \"\"\n",
    "for i in range(min(5, num_pages)):  # First 5 pages or all if less than 5\n",
    "    page = pdf_reader.pages[i]\n",
    "    full_text += page.extract_text() + \"\\n\\n\"\n",
    "    \n",
    "# Limit text length to avoid token limits\n",
    "max_length = 10000\n",
    "if len(full_text) > max_length:\n",
    "    full_text = full_text[:max_length] + \"...\"\n",
    "\n",
    "print(f\"Extracted {len(full_text)} characters of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad50dc0",
   "metadata": {},
   "source": [
    "Summarise a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a3ffb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided text, here's a breakdown of the Gemini 1.5 Pro training process, focusing on what's explicitly mentioned:\n",
       "\n",
       "**Key Aspects of the Training Process:**\n",
       "\n",
       "*   **Model Architecture:**\n",
       "    *   **Mixture-of-Experts (MoE):** Gemini 1.5 Pro is a sparse Mixture-of-Experts (MoE) model. This means it uses a learned routing function to direct inputs to a subset of the model's parameters for processing. This allows the model to have a large total parameter count while keeping the number of parameters activated for any given input constant, improving efficiency.\n",
       "    *   **Transformer-based:** It builds upon the Transformer architecture, which is a standard in modern language models.\n",
       "*   **Data:**\n",
       "    *   **Multimodal:** The model is trained on multimodal data, including text, video, and audio.\n",
       "    *   **Long Context:** The training data includes very long sequences, up to 10 million tokens. This is a key factor in enabling the model's long-context understanding capabilities.\n",
       "*   **Optimization:**\n",
       "    *   The text mentions \"a host of improvements made across nearly the entire model stack (architecture, data, optimization and systems)\". This suggests that various optimization techniques were employed to improve the model's performance and efficiency.\n",
       "*   **Compute Efficiency:**\n",
       "    *   A major goal was to achieve comparable quality to Gemini 1.0 Ultra while using significantly less training compute and being more efficient to serve.\n",
       "*   **Long-Context Understanding:**\n",
       "    *   The model incorporates architectural changes specifically designed to enable long-context understanding of inputs up to 10 million tokens without degrading performance.\n",
       "*   **Comparison to Previous Models:**\n",
       "    *   The model builds on the research advances and multimodal capabilities of Gemini 1.0.\n",
       "    *   It also draws upon a longer history of MoE research at Google and language model research in the broader literature.\n",
       "\n",
       "**In summary, the training process for Gemini 1.5 Pro involved:**\n",
       "\n",
       "1.  **Leveraging a Mixture-of-Experts (MoE) Transformer architecture.**\n",
       "2.  **Training on a diverse multimodal dataset with extremely long contexts (up to 10 million tokens).**\n",
       "3.  **Employing various optimization techniques to improve performance and efficiency.**\n",
       "4.  **Incorporating architectural changes to specifically enhance long-context understanding.**\n",
       "5.  **Aiming for comparable quality to Gemini 1.0 Ultra with significantly less training compute.**\n",
       "\n",
       "The text emphasizes the importance of the MoE architecture and the long-context training data in achieving the model's impressive capabilities.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = 'Tell me about the training process used here.'\n",
    "\n",
    "def summarise_doc(request: str) -> str:\n",
    "  \"\"\"Execute the request on the uploaded document.\"\"\"\n",
    "  # Set the temperature low to stabilise the output.\n",
    "  config = types.GenerateContentConfig(temperature=0.0)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[request, full_text],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "summary = summarise_doc(request)\n",
    "Markdown(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286c83a",
   "metadata": {},
   "source": [
    "**Define an evaluator**\n",
    "For a task like this, you may wish to evaluate a number of aspects, like how well the model followed the prompt (\"instruction following\"), whether it included relevant data in the prompt (\"groundedness\"), how easy the text is to read (\"fluency\"), or other factors like \"verbosity\" or \"quality\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f03f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "# Define the evaluation prompt\n",
    "SUMMARY_PROMPT = \"\"\"\\\n",
    "# Instruction\n",
    "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
    "We will provide you with the user input and an AI-generated responses.\n",
    "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n",
    "\n",
    "## Criteria\n",
    "Instruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.\n",
    "Groundedness: The response contains information included only in the context. The response does not reference any outside information.\n",
    "Conciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.\n",
    "Fluency: The response is well-organized and easy to read.\n",
    "\n",
    "## Rating Rubric\n",
    "5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.\n",
    "4: (Good). The summary follows instructions, is grounded, concise, and fluent.\n",
    "3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.\n",
    "2: (Bad). The summary is grounded, but does not follow the instructions.\n",
    "1: (Very bad). The summary is not grounded.\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.\n",
    "STEP 2: Score based on the rubric.\n",
    "\n",
    "# User Inputs and AI-generated Response\n",
    "## User Inputs\n",
    "\n",
    "### Prompt\n",
    "{prompt}\n",
    "\n",
    "## AI-generated Response\n",
    "{response}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e9675e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Evaluation\n",
       "STEP 1:\n",
       "The response successfully follows the instruction by providing information regarding the training process of Gemini 1.5 Pro, based on the given document. It effectively summarizes the key aspects of the training process. The response is grounded and only uses information present in the context. The response is quite verbose and could be more concise by avoiding repetition and unnecessary introductory phrases.\n",
       "\n",
       "STEP 2:\n",
       "Rating: 4 (Good). The summary follows instructions, is grounded, concise, and fluent."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a structured enum class to capture the result.\n",
    "class SummaryRating(enum.Enum):\n",
    "  VERY_GOOD = '5'\n",
    "  GOOD = '4'\n",
    "  OK = '3'\n",
    "  BAD = '2'\n",
    "  VERY_BAD = '1'\n",
    "\n",
    "\n",
    "def eval_summary(prompt, ai_response):\n",
    "  \"\"\"Evaluate the generated summary against the prompt used.\"\"\"\n",
    "\n",
    "  chat = client.chats.create(model='gemini-2.0-flash')\n",
    "\n",
    "  # Generate the full text response.\n",
    "  response = chat.send_message(\n",
    "      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)\n",
    "  )\n",
    "  verbose_eval = response.text\n",
    "\n",
    "  # Coerce into the desired structure.\n",
    "  structured_output_config = types.GenerateContentConfig(\n",
    "      response_mime_type=\"text/x.enum\",\n",
    "      response_schema=SummaryRating,\n",
    "  )\n",
    "  response = chat.send_message(\n",
    "      message=\"Convert the final score.\",\n",
    "      config=structured_output_config,\n",
    "  )\n",
    "  structured_eval = response.parsed\n",
    "\n",
    "  return verbose_eval, structured_eval\n",
    "\n",
    "\n",
    "text_eval, struct_eval = eval_summary(prompt=[request, full_text], ai_response=summary)\n",
    "Markdown(text_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec18ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SummaryRating.GOOD: '4'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
